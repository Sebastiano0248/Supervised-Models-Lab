{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4820d85d-0991-4b35-bcf9-737c3ddc32f3",
   "metadata": {},
   "source": [
    "# PAC learning in 2D rectangular classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047e3ab-7c81-4131-a08b-3bb385b45fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa7e7b-09f6-4a31-98b9-4b0813da638f",
   "metadata": {},
   "source": [
    "Let's create the function that defines the real classification rule which, in this case, is a rectangle (left bottom and right upper points are enough to define it)\n",
    "\n",
    "<img src=\"images/rect1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba6f52-f009-4c0a-95b5-5de0482eae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,points):\n",
    "    if x[0] >= points[0,0] and x[0] <= points[1,0] and \\\n",
    "       x[1] >= points[0,1] and x[1] <= points[1,1]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143cc1d-9e83-4cdc-9260-d13d1623379b",
   "metadata": {},
   "source": [
    "Let's define the X space and two points of the classification rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa6cb8-3ec3-432a-b2d9-ddbb48580ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[4,2],\n",
    "                   [6.9,8]])\n",
    "xlims = [0,10]\n",
    "ylims = [0,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37465b3b-f4d4-4b2b-bfbf-e8ed740aea1c",
   "metadata": {},
   "source": [
    "Now, we create the dataset with random-sampled descriptive feature `X`(2 dimensions, `x` and `y`) and the class variable `c`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817931f-edf5-4d95-9bd4-74fa71ff91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "N=100\n",
    "X = np.array([np.random.uniform(xlims[0],xlims[1],N),\n",
    "              np.random.uniform(ylims[0],ylims[1],N)]).T\n",
    "c = np.array([f(X[i,:],points) for i in range(N)]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593ef53-79a1-4685-8ce6-7b39ebf1322d",
   "metadata": {},
   "source": [
    "## Learning\n",
    "The learning method is as simple as finding two points **among the positive samples**: \n",
    "- The left-bottom point is composed of: (the smallest `x` value, the smallest `y` value)\n",
    "- The right-upper point is composed of: (the largest `x` value, the largest `y` value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f88fae-594e-4c86-8741-1a27169c852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_method(X,c):\n",
    "    idx_pos = np.where(c==1)[0]\n",
    "    pos_samples = X[idx_pos,:]\n",
    "    points = np.array([np.min(pos_samples, axis=0),\n",
    "                      np.max(pos_samples, axis=0)])\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a621e7-18f4-4046-97d7-b1329d50886d",
   "metadata": {},
   "source": [
    "Let's learn our classification rule, and then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10232a-0ba5-4c19-9468-103ee16648c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_points = learning_method(X,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5222e9f-febe-4e1f-9159-4bcc91708d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=c)\n",
    "plt.gca().add_patch(\n",
    "    patches.Rectangle(\n",
    "        xy=points[0,:],  # point of origin.\n",
    "        width=points[1,0]-points[0,0], height=points[1,1]-points[0,1], linewidth=1, linestyle=\"dotted\",\n",
    "        color='green', fill=False))\n",
    "\n",
    "plt.gca().add_patch(\n",
    "    patches.Rectangle(\n",
    "        xy=learned_points[0,:],  # point of origin.\n",
    "        width=learned_points[1,0]-learned_points[0,0], \n",
    "        height=learned_points[1,1]-learned_points[0,1], \n",
    "        linewidth=1, linestyle=\"solid\", color='red', fill=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d7442-4085-434f-8d7b-a551a0ea3777",
   "metadata": {},
   "source": [
    "Note than in this scenario we can quantify the error of the learned rule as the area of the real classification rule not covered by the learned one. Let's draw it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5172c-0479-46fe-a9b3-5e58e698e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=c)\n",
    "plt.gca().add_patch(\n",
    "    patches.Rectangle(\n",
    "        xy=points[0,:],  # point of origin.\n",
    "        width=points[1,0]-points[0,0], height=points[1,1]-points[0,1], linewidth=1, linestyle=\"dotted\",\n",
    "        color='green', fill=False))\n",
    "\n",
    "plt.gca().add_patch(\n",
    "    patches.Rectangle(\n",
    "        xy=learned_points[0,:],  # point of origin.\n",
    "        width=learned_points[1,0]-learned_points[0,0], \n",
    "        height=learned_points[1,1]-learned_points[0,1], \n",
    "        linewidth=1, linestyle=\"solid\", color='red', fill=False))\n",
    "\n",
    "plt.gca().add_patch( #left error space (RED)\n",
    "    patches.Rectangle(\n",
    "        xy=points[0,:],  # point of origin.\n",
    "        width=learned_points[0,0]-points[0,0],  #x\n",
    "        height=points[1,1]-points[0,1], #y\n",
    "        linewidth=1, linestyle=\"solid\", color='red', fill=True, alpha=0.25))\n",
    "\n",
    "plt.gca().add_patch( #bottom error space (GREEN)\n",
    "    patches.Rectangle(\n",
    "        xy=(learned_points[0,0],points[0,1]),  # point of origin.\n",
    "        width=learned_points[1,0]-learned_points[0,0],  #x\n",
    "        height=learned_points[0,1]-points[0,1], #y\n",
    "        linewidth=1, linestyle=\"solid\", color='green', fill=True, alpha=0.25))\n",
    "\n",
    "plt.gca().add_patch( #upper error space (BLUE)\n",
    "    patches.Rectangle(\n",
    "        xy=(learned_points[0,0],learned_points[1,1]),  # point of origin.\n",
    "        width=learned_points[1,0]-learned_points[0,0],  #x\n",
    "        height=points[1,1]-learned_points[1,1], #y\n",
    "        linewidth=1, linestyle=\"solid\", color='blue', fill=True, alpha=0.25))\n",
    "\n",
    "plt.gca().add_patch( #upper error space (GRAY)\n",
    "    patches.Rectangle(\n",
    "        xy=(learned_points[1,0],points[0,1]),  # point of origin.\n",
    "        width=points[1,0]-learned_points[1,0],  #x\n",
    "        height=points[1,1]-points[0,1], #y\n",
    "        linewidth=1, linestyle=\"solid\", color='yellow', fill=True, alpha=0.25))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71833dfc-95d6-46f0-9790-eb13feca5407",
   "metadata": {},
   "source": [
    "For convenience, the error area is decomposed into 4 rectangles. Computing the error of the classifier is just:\n",
    "1. calculating the area of each of these 4 error areas, and adding them up,\n",
    "2. calculating the area of the whole input space,\n",
    "3. dividing the error area by the input space area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1224d93-067f-419c-b651-e23e17f543a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area(p1,p2):\n",
    "    return np.abs((p2[0]-p1[0])*(p2[1]-p1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22300ca7-b810-4ade-9be3-462a0b7c0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_error(real_sq, pred_sq, xlims, ylims):\n",
    "    err_area  = area(real_sq[0,:],(pred_sq[0,0],real_sq[1,1])) # left\n",
    "    err_area += area((pred_sq[0,0],real_sq[0,1]), (pred_sq[1,0],pred_sq[0,1]))#bottom\n",
    "    err_area += area((pred_sq[0,0],pred_sq[1,1]), (pred_sq[1,0],real_sq[1,1]))#upper\n",
    "    err_area += area((pred_sq[1,0],real_sq[0,1]), real_sq[1,:]) #right\n",
    "    \n",
    "    total_area = area((xlims[0],ylims[0]),(xlims[1],ylims[1]))\n",
    "\n",
    "    return err_area/total_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20565b5-717a-4acc-b580-4cae9ffcc619",
   "metadata": {},
   "source": [
    "So, the error of the classifier is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632dc12-b4f0-4451-8948-f379caf1887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_error(points, learned_points, xlims, ylims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0e46a-aa82-4093-9f76-7711cb0e9115",
   "metadata": {},
   "source": [
    "## Using PAC bounds to provide a minimum sample size\n",
    "The PAC learning theory establishes the minimum number of samples to guarantee a given maximum allowed error with at most a given probability value $\\delta$ with the following formula:\n",
    "\n",
    "$$N\\geq \\frac{4}{\\epsilon}\\log{\\frac{4}{\\delta}}$$\n",
    "\n",
    "<sub><sup>(the derivation of this expression is in the lecture's material)</sup></sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cedeff-1042-4395-a486-7d8c7c560b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pac_sample_complexity(eps, delta):\n",
    "    return 4/eps * np.log(4/delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae596959-4ec2-42b9-bb45-8dde618ea5c8",
   "metadata": {},
   "source": [
    "So, if we want to allow at most 10% of error at most in 5% of times we learn a classifier, the result is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccdf091-3ea4-43fe-b6bb-1adcfa093499",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "delta = 0.05\n",
    "int(np.ceil(pac_sample_complexity(eps, delta))) # N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e4e4f-b99c-44f7-8ff3-789b1c28deeb",
   "metadata": {},
   "source": [
    "Let's now make an experiment with different $\\epsilon$ and $\\delta$ values. For each combination, we will repeat the data generation and learning processes a given number of times (`nreps`) to study whether the learning guarantees hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e13f5-aba5-4f21-ad30-2f8ac0c26164",
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_eps = [0.3,0.2,0.1,0.05,0.01]\n",
    "considered_delta = [0.1,0.05,0.01]\n",
    "nreps = 1000 # run each experiment this amount of times\n",
    "factor_of_pac_bound = 1\n",
    "\n",
    "res = np.zeros((len(considered_eps),len(considered_delta)))\n",
    "for ie, eps in enumerate(considered_eps):\n",
    "    for id, delta in enumerate(considered_delta):\n",
    "        N = int(np.ceil(pac_sample_complexity(eps, delta)*factor_of_pac_bound))\n",
    "        errs = []\n",
    "        for r in np.arange(nreps):\n",
    "            c=np.zeros(1)\n",
    "            while np.sum(c)==0:\n",
    "                X = np.array([np.random.uniform(xlims[0],xlims[1],N),\n",
    "                              np.random.uniform(ylims[0],ylims[1],N)]).T\n",
    "                c = np.array([f(X[i,:],points) for i in range(N)]).astype(int)\n",
    "            \n",
    "            learned_points = learning_method(X,c)\n",
    "\n",
    "            err = measure_error(points, learned_points, xlims, ylims)\n",
    "            errs.append(err)\n",
    "        res[ie,id] = len(np.where(np.array(errs)<eps)[0])/nreps # proportion of cases in which epsilon-bound holds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9dbe23-bec0-40c8-8dff-4938217dfc64",
   "metadata": {},
   "source": [
    "These are the results of the experiment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff35f89-b6fb-4bd4-8b11-48097bbcf7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbfd32-9896-49e8-9cdc-4eb56ea55ee6",
   "metadata": {},
   "source": [
    "showing the proportion of repetitions in which the epsilon bounds hold (for different $\\epsilon$ values --per row-- and different $\\delta$ values --per column)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e21b0d-fed0-4ff7-9042-61d984681c35",
   "metadata": {},
   "source": [
    "## Questions\n",
    "- What do we observe within these results? What should we observe? What can you say about the tightness of the PAC bounds for this problem?\n",
    "- Play a bit with `factor_of_pac_bound` $(0\\leq factor\\leq 1)$ to test different scenarios where we actually use a fewer number of samples regarding the PAC-based suggested sample size.\n",
    "- Which would be the bound based on the VCdim? Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd89d43-0a8d-4ed3-a267-b497d585cbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
