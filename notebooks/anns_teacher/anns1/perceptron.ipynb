{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmgrHTazuap0"
   },
   "source": [
    "# Perceptron\n",
    "In this notebook, we will see the basic operative of the **perceptron**, the most basic neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1614301542770,
     "user": {
      "displayName": "Luis Cruz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAdeJfpdmWlDnaYzYy_nbUNs-8KmTX4RuBuPQbP_4=s64",
      "userId": "01516691432947049938"
     },
     "user_tz": 360
    },
    "id": "SFE6QWLFuaqp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron, in its most basic version, is built by a single neuron with a step activation function:\n",
    "\n",
    "$$f({\\bf x}) = \\phi\\Big( \\sum_{i=1}^n(w_i\\cdot x_i)-b\\Big)$$\n",
    "with\n",
    "$$\\phi(x)=\n",
    "\\begin{cases}\n",
    "  1 & \\text{if $x>0$} \\\\\n",
    "  0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where we combine the input of the neuron with the parameters throughout the <a href=\"https://en.wikipedia.org/wiki/Dot_product\" _target=\"blank\">*inner product*</a> (linear combination of inputs) and apply the activation function. For convenience, in the next code we define these two steps in separate functions of the class `neuron`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dqFjSUKuaqk"
   },
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    def __init__(self, W, b, act_func):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.act_func = act_func\n",
    "\n",
    "    def linear_combination(self, vector):\n",
    "        return np.dot(self.W,vector)-self.b\n",
    "\n",
    "    def compute(self, vector):\n",
    "        return self.act_func(self.linear_combination(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    return 1 if x>0 else 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OR and AND operators\n",
    "AND and OR are simple mathematical functions that can also be represented by a simple neural network (perceptron). In both cases, there are two operands (two dimensions, $n=2$), but it requires a specific configuration of the parameters:\n",
    "- $w_1=w_2=1$\n",
    "- $b=1.5$\n",
    "\n",
    "for **and** operator.\n",
    "\n",
    "<img src=\"images/operator_and.png\" alt=\"and\" style=\"width: 200px;\"/>\n",
    "\n",
    "We can set up now the neuron with the configuration mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_and = neuron(np.ones(2),1.5,step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and test it. Does it work as the **AND** operator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1]:\n",
    "    for j in [0,1]:\n",
    "        print(\"Result of AND(\",i,\",\",j,\"):\", f_and.compute(np.array([i,j])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **or** operator, the configuration of the parameters is:\n",
    "- $w_1=w_2=1$\n",
    "- $b=0.5$\n",
    "\n",
    "<img src=\"images/operator_or.png\" alt=\"or\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_or = neuron(np.ones(2),.5,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1]:\n",
    "    for j in [0,1]:\n",
    "        print(\"Result of OR(\",i,\",\",j,\"):\", f_or.compute(np.array([i,j])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work as expected, isn't it?\n",
    "\n",
    "But let's explore HOW it works. Let's see the output space of the linear combination of the AND's neuron, and its transformation once the activation function is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logical_func(model): \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    x = np.linspace(-0.1, 1.1, 100)\n",
    "    y = np.linspace(-0.1, 1.1, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    xs = np.array([0,0,1,1])\n",
    "    ys = np.array([0,1,0,1])\n",
    "    \n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    \n",
    "    Zb = np.array([model.linear_combination(np.array([xi, yi])) for xi, yi in zip(X.ravel(), Y.ravel())])\n",
    "    Zb = Zb.reshape(X.shape)  # Reshape to match X and Y grid shapes\n",
    "    zbs = np.array([model.linear_combination(np.array([xi, yi])) for xi, yi in zip(xs, ys)])\n",
    "    \n",
    "    ax.plot_surface(X, Y, Zb, cmap='viridis', alpha=0.5)\n",
    "    ax.scatter(xs, ys, zbs)#, marker=m)\n",
    "    ax.set_title(\"Before activation\")\n",
    "    \n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    Za = np.array([model.compute(np.array([xi, yi])) for xi, yi in zip(X.ravel(), Y.ravel())])\n",
    "    Za = Za.reshape(X.shape)  # Reshape to match X and Y grid shapes\n",
    "    zas = np.array([model.compute(np.array([xi, yi])) for xi, yi in zip(xs, ys)])\n",
    "    \n",
    "    ax.plot_surface(X, Y, Za, cmap='viridis', alpha=0.5)\n",
    "    ax.scatter(xs, ys, zas)#, marker=m)\n",
    "    ax.set_title(\"After activation\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logical_func(f_and)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 points represent the output of the logical function given all the combination of 2 binary inputs.\n",
    "\n",
    "For the OR operator, we observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logical_func(f_or)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots, one can easily observe the non-linearity introduced by the activation functions to the linear combination the inputs.\n",
    "\n",
    "## Questions\n",
    "- Which is the effect of changing the value of the bias term, $b$?\n",
    "- Can we configure a single-neuron model (perceptron) to simulate the XOR-operator? Why?\n",
    "\n",
    "<img src=\"images/operator_xor.png\" alt=\"xor\" style=\"width: 200px;\"/>\n",
    "\n",
    "Let's build a small neural network model with a single hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hidden-layer NN model\n",
    "class nn_model:\n",
    "    def __init__(self, hidden_layer, output_layer):\n",
    "        self.hl = hidden_layer\n",
    "        self.ol = output_layer\n",
    "\n",
    "    def linear_combination(self, vector):\n",
    "        hl_out = np.array([n.linear_combination(vector) for n in self.hl])\n",
    "        # The output of a layer is the input of the following one\n",
    "        ol_out = np.array([n.linear_combination(hl_out) for n in self.ol])\n",
    "        return [hl_out, ol_out]\n",
    "        \n",
    "    def compute(self, vector):\n",
    "        hl_out = np.array([n.compute(vector) for n in self.hl])\n",
    "        # The output of a layer is the input of the following one\n",
    "        ol_out = np.array([n.compute(hl_out) for n in self.ol])\n",
    "        return [hl_out, ol_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the model for the XOR operator, using a first layer with two neurons (defined just as AND and OR operators) and a single neuron in the second (output) layer, parametrized as follows:\n",
    "- $w_1=-1$: this is the parameter controlling the contribution of the AND-operator's output\n",
    "- $w_2=1$: parameter for OR-operator's output\n",
    "- $b=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_xor = nn_model([f_and, f_or], [neuron(np.array([-1,1]),.5,step)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1]:\n",
    "    for j in [0,1]:\n",
    "        print(\"Result of XOR(\",i,\",\",j,\"):\", f_xor.compute(np.array([i,j]))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "x = np.linspace(-0.1, 1.1, 100)\n",
    "y = np.linspace(-0.1, 1.1, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xs = np.array([0,0,1,1])\n",
    "ys = np.array([0,1,0,1])\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "Zb = np.array([f_xor.linear_combination(np.array([xi, yi]))[1][0] for xi, yi in zip(X.ravel(), Y.ravel())])\n",
    "Zb = Zb.reshape(X.shape)  # Reshape to match X and Y grid shapes\n",
    "zbs = np.array([f_xor.linear_combination(np.array([xi, yi]))[1][0] for xi, yi in zip(xs, ys)])\n",
    "\n",
    "ax.plot_surface(X, Y, Zb, cmap='viridis', alpha=0.5)\n",
    "ax.scatter(xs, ys, zbs)#, marker=m)\n",
    "ax.set_title(\"Only linear combinations\")\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "Za = np.array([f_xor.compute(np.array([xi, yi]))[1][0] for xi, yi in zip(X.ravel(), Y.ravel())])\n",
    "Za = Za.reshape(X.shape)  # Reshape to match X and Y grid shapes\n",
    "zas = np.array([f_xor.compute(np.array([xi, yi]))[1][0] for xi, yi in zip(xs, ys)])\n",
    "\n",
    "ax.plot_surface(X, Y, Za, cmap='viridis', alpha=0.5)\n",
    "ax.scatter(xs, ys, zas)#, marker=m)\n",
    "ax.set_title(\"With non-linear transformations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe in the previous plots that exclusively relying in the linear combinations we could not have the expected result. The activation function introduces non-linearities in the network that allows for identifying patterns.\n",
    "\n",
    "## Questions:\n",
    "- Why do we need more than one layer of neurons?\n",
    "- It is said that all neurons in a NN model would collapse into a single neuron if they just compute linear combinations. Could you tell why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algebra\n",
    "\n",
    "In this introduction to neural networks we have presented a model as a set of operations that are executed one-by-one (all the neurons of a layer can be executed in parallel, neurons from different layers, sequentially).\n",
    "\n",
    "However, we can see NNs as a set of <a href=\"https://en.wikipedia.org/wiki/Linear_algebra\" target=\"blank\">algebra calculations</a>:\n",
    "- we have already presented the input of neurons and the weights of each neuron as vectors.\n",
    "- the weights of the neurons in the same layer can be seen as rows of a matrix $W_l$, and the corresponding bias terms as elements of a vector $\\mathbf{b}_l$ such that, given layer $l$'s input $\\mathbf{x}_l$, the output of the layer is the vector:\n",
    "\n",
    "$$\\mathbf{o}_l=\\phi(W_l\\cdot\\mathbf{x}_l-\\mathbf{b}_l)$$\n",
    "\n",
    "- the concatenation of layers can be seen as the concatenation of such calculations where the input of each layer is the output of the previous one ($\\mathbf{x}_l = \\mathbf{o}_{l-1}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1614301543167,
     "user": {
      "displayName": "Luis Cruz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAdeJfpdmWlDnaYzYy_nbUNs-8KmTX4RuBuPQbP_4=s64",
      "userId": "01516691432947049938"
     },
     "user_tz": 360
    },
    "id": "mj81KRsDuaq8",
    "outputId": "db2022f1-d906-4d99-f7f0-b99d7d4a338f"
   },
   "outputs": [],
   "source": [
    "# a version of the step function which can lead with vector\n",
    "def step(x):\n",
    "    return (x>0).astype(float)\n",
    "\n",
    "# one hidden-layer NN model as a concatenation of linear algebra operations\n",
    "class nn_model_alg:\n",
    "    def __init__(self, weights, bias_terms, f):\n",
    "        self.W = weights\n",
    "        self.b = bias_terms\n",
    "        self.phi = f\n",
    "        \n",
    "    def compute(self, x):\n",
    "        x_new = x.copy()\n",
    "        for W, b in zip(self.W, self.b):\n",
    "            x_new = self.phi(np.matmul(W,x_new)-b)\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the network for the XOR operator, formated as explained above, would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.ones((2,2)) # weights of the hidden layer (2 neurons) as rows\n",
    "b0 = np.array([[1.5,0.5]]).T # as a column\n",
    "W1 = np.array([[-1.,1.]]) # a row\n",
    "b1 = np.array([[0.5]])\n",
    "\n",
    "f_xor = nn_model_alg([W0,W1],[b0,b1],step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1]:\n",
    "    for j in [0,1]:\n",
    "        print(\"Result of XOR(\",i,\",\",j,\"):\", f_xor.compute(np.array([[i,j]]).T)[0,0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IA_Notebook _4_Programando_Red_Neuronal_desde_Cero.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
