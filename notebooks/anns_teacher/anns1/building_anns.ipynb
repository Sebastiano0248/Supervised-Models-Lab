{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmgrHTazuap0"
   },
   "source": [
    "# Building an Artificial Neural Network\n",
    "In this notebook, we will write our own code for ANNs and implement the algorithm of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1614301542770,
     "user": {
      "displayName": "Luis Cruz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAdeJfpdmWlDnaYzYy_nbUNs-8KmTX4RuBuPQbP_4=s64",
      "userId": "01516691432947049938"
     },
     "user_tz": 360
    },
    "id": "SFE6QWLFuaqp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_cyttlCiB4k"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will use `sklearn`'s `make_circles` to create a dataset with 2-D points forming two concentric circles. `X` saves the 2D coordinates for all the points, and `y` is the vector determining which circle each point belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1614301543167,
     "user": {
      "displayName": "Luis Cruz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAdeJfpdmWlDnaYzYy_nbUNs-8KmTX4RuBuPQbP_4=s64",
      "userId": "01516691432947049938"
     },
     "user_tz": 360
    },
    "id": "mj81KRsDuaq8",
    "outputId": "db2022f1-d906-4d99-f7f0-b99d7d4a338f"
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "from sklearn import datasets\n",
    "\n",
    "n = 500\n",
    "p = 2\n",
    "\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, factor=0.6, noise = 0.05, random_state=13)\n",
    "y = y[:, np.newaxis] #convertim y into a column vector\n",
    "c = 2\n",
    "#X, y = datasets.make_moons(n_samples=500, shuffle=True, noise=0.05, random_state=13)\n",
    "#c = 2\n",
    "#c = 4\n",
    "#X, y = datasets.make_blobs(n_samples=500, shuffle=True, centers=c, random_state=13)\n",
    "\n",
    "print(\"Dimensions de X:\",X.shape)\n",
    "print(\"Dimensions de Y:\",y.shape)\n",
    "\n",
    "colors = iter(cm.rainbow(np.linspace(0, 1, c)))\n",
    "\n",
    "for yp in np.arange(c):\n",
    "    yp_points = y[:,0] == yp\n",
    "    plt.scatter(X[yp_points, 0],X[yp_points, 1], c=np.array([next(colors)]) )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtAwgmsGuarb"
   },
   "source": [
    "## Neural network's layer (class `nn_layer`)\n",
    "\n",
    "We can create a class to store the elements of each layer:\n",
    "- a matrix of weights `W` of size ($M\\times N$), where $M$ is the number of neurons in the previous layer (inputs to each neuron in the current layer) and $N$ is the number of neurons in the current layer.\n",
    "- a vector of bias terms `b` of size ($1\\times N$)\n",
    "- the activation function (and its derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1592,
     "status": "ok",
     "timestamp": 1614301543634,
     "user": {
      "displayName": "Luis Cruz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAdeJfpdmWlDnaYzYy_nbUNs-8KmTX4RuBuPQbP_4=s64",
      "userId": "01516691432947049938"
     },
     "user_tz": 360
    },
    "id": "DBZO4NLzuare"
   },
   "outputs": [],
   "source": [
    "class NNLayer:\n",
    "    '''\n",
    "    Create random weights and bias terms in the interval [-1,1].\n",
    "    m: number of neurons in the previous layer (inputs to each neuron in the current layer)\n",
    "    n: number of neurons in the current layer\n",
    "    act_f: activation function (act_f[0]) and its derivative (act_f[1])\n",
    "    '''\n",
    "    def __init__(self, m, n, act_f,rnd_gen=np.random.default_rng()):\n",
    "        self.b = rnd_gen.random((1, n))*2-1\n",
    "        self.W = rnd_gen.random((m, n))*2-1\n",
    "        self.act_f = act_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l597jKIXxDhy"
   },
   "source": [
    "## Neural network\n",
    "\n",
    "We can create a neural network as a sequence of layers. We only need to know the number of layers, the number of neurons in each one, and the corresponding activation function.\n",
    "\n",
    "El vector ```topology``` que és una llista de capes (0 l'inicial o entrada, 1..n-2 les ocultes i n-1 la final o sortida), cada element del vector té una tupla formada per:\n",
    "        \n",
    "1. El número de neurones de cada capa\n",
    "\n",
    "2. La Funció d'activació de la capa.\n",
    "\n",
    "El resultat serà una llista de ```neural_layer```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2189,
     "status": "ok",
     "timestamp": 1614301544347,
     "user": {
      "displayName": "Luis Cruz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAdeJfpdmWlDnaYzYy_nbUNs-8KmTX4RuBuPQbP_4=s64",
      "userId": "01516691432947049938"
     },
     "user_tz": 360
    },
    "id": "mhYgAZMtuarz"
   },
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    '''\n",
    "    Create a NN\n",
    "    in_dim: input dimension\n",
    "    n_neurons: vector with the number of neurons in each layer\n",
    "    act_fs: activation function for each layer\n",
    "    '''\n",
    "    def __init__(self, in_dim, n_neurons, act_fs, rnd_gen=np.random.default_rng()):\n",
    "        self.layers = []\n",
    "        self.layers.append(NNLayer(in_dim,n_neurons[0],act_fs[0], rnd_gen))\n",
    "        for l in range(1,len(n_neurons)):\n",
    "            self.layers.append(NNLayer(n_neurons[l-1], n_neurons[l] , act_fs[l], rnd_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    " \n",
    "Forward propagation is nothing more than running the ANN with input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Performs forward propagation\n",
    "X: input data (n,m): where n is the number of cases, and m the input dimension\n",
    "store_all: stores and returns intermediate results\n",
    "'''\n",
    "def forward_propagation(self, X, store_all=True):\n",
    "    # We might want to store all the intermediate results to reuse later (backpropagation)\n",
    "    self.intermediate_results = []\n",
    "    if store_all:\n",
    "        self.intermediate_results.append((None,X))\n",
    "\n",
    "    prev_out = X\n",
    "    for l in range(len(self.layers)):\n",
    "        # We first calculate the linear combination\n",
    "        # @ operator is the matrix multiplication\n",
    "        # the final z matrix will be size (n_cases x n_neurons)\n",
    "        z = prev_out @ self.layers[l].W + self.layers[l].b\n",
    "\n",
    "        # We apply the activation function\n",
    "        # the final matrix, a, will be size (n_cases x n_neurons)\n",
    "        a = #### YOUR CODE HERE ####\n",
    "\n",
    "        prev_out = a\n",
    "        if store_all:\n",
    "            self.intermediate_results.append((z,a))\n",
    "\n",
    "    return prev_out\n",
    "\n",
    "# add method to the ANN class\n",
    "ANN.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d915UIdtuarg"
   },
   "source": [
    "Now we have defined the basic operative of neural networks: the configuration and input processing.\n",
    "\n",
    "Let's assume we need an ANN with two hidden layers and an output layer, where both hidden layers have 3 neurons each and the output just one neuron, such as:\n",
    "\n",
    "<img src=\"images/ann_graph.png\" alt=\"ann graph\" style=\"width: 600px;\"/>\n",
    "\n",
    "Each neuron linearly combines the inputs and applies a non-linear transformation $\\sigma$ to it.\n",
    "\n",
    "$$\\phi_j^{(l)}(\\textbf{a}^{(l-1)})=\\sigma\\left( \\sum_{k=1}^{N^{(l-1)}} w^{(l)}_{kj}\\cdot a^{(l-1)}_{k}+ b^{(l)}_j\\right)$$\n",
    "\n",
    "for the $j$-th neuron of layer $l$, where $\\textbf{a}^{(l-1)}$ is the vector of outputs of the previous layer (length $N^{(l-1)}$). The input vector $\\textbf{a}^{(l-1)}$ is $(\\phi_j^{(l)})_{j=1}^{N^{(l-1)}}$ if $l-1>1$, or just the input data $\\textbf{x}$  if $l-1=1$.\n",
    "The function $\\sigma$ is usually the same for each layer's neurons.\n",
    "\n",
    "**Note that** these steps are repeatedly applied for all the neurons and layers. Thus, we can represent them in matrix form:\n",
    "\n",
    "$$\\textbf{Z}^{(l)}=\\textbf{A}^{(l-1)}\\times \\textbf{W}^{(l)}+\\textbf{b}^{(l)}$$\n",
    "\n",
    "where $\\times$ means <a href=\"https://en.wikipedia.org/wiki/Matrix_multiplication\" target=\"blank\">matrix multiplication</a> and it sums the bias vector $\\textbf{b}^{(l)}$ to each row of the resulting matrix. From this matrix $\\textbf{Z}^{(l)}$, the layer $l$'s output matrix:\n",
    "$$\\textbf{A}^{(l)}=\\sigma\\left(\\textbf{Z}^{(l)}\\right)$$\n",
    "is just the element-wise application of function $\\sigma$ to $\\mathbf{Z}^{(l)}$, $A^{(l)}_{jk}=\\sigma\\left(Z^{(l)}_{jk}\\right)$.\n",
    "\n",
    "The sequence of vectors and matrices required in the whole computation can be graphically represented as:\n",
    "<img src=\"images/ann_matrix.png\" alt=\"ann matrix\" style=\"width:1200px;\"/>\n",
    "\n",
    "\n",
    "Remember that we have left a basic component undefined: the activation function $\\sigma$. This is the non-linear transformation that is applied to each linear combination in the neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d915UIdtuarg"
   },
   "source": [
    "## Activation functions\n",
    "\n",
    "Let us define the most typical activation functions, starting with the hyperbolic tangent `tanh`.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$\\phi (z)= \\text{tanh}(z) =  \\frac{e^{z}-e^{-z}}{e^{-z}+e^{z}}$$\n",
    "\n",
    "and its first derivative is:\n",
    "$$\\phi' (z)= \\text{tanh}'(z) = 1 - \\phi^{2}(z) = \\frac{4}{(e^{-z}+e^{z})^{2}}$$\n",
    "\n",
    "We will save them in the same tuple, where the first element is the function itself, and the second element is its first derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = ( lambda z : ((np.e**z)-(np.e**(-z)))/((np.e**(-z))+(np.e**z)),  # function\n",
    "         lambda z : 4./(((np.e**z)+(np.e**(-z)))**2) )                  # first derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d915UIdtuarg"
   },
   "source": [
    "The `sigmoid` function is also a standard one. It is defined as:\n",
    "\n",
    "$$\\phi (z)= \\text{sigmoid}(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "and its first derivative is:\n",
    "\n",
    "$$\\phi' (z)= \\text{sigmoid}'(z) = \\phi (z) (1 - \\phi(z)) = \\frac{e^{-z}}{(1+e^{-z})^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = ( lambda z : 1./(1.+(np.e**(-z))),                  # function\n",
    "            lambda z : #### YOUR CODE HERE #### )  # first derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d915UIdtuarg"
   },
   "source": [
    "A modern popular activation function is the Rectified Linear Unit (`ReLu`), defined as:\n",
    "\n",
    "$$\\phi (z)= \\text{ReLu}(z) = \\max{(0,z)}$$\n",
    "\n",
    "and its first derivative is: \n",
    "\n",
    "$$\\phi' (z)= \\text{ReLu}'(z) = \\begin{cases} 0, & \\text{if } z\\leq 0 \\\\ 1, & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Note that it is really undefined at $z=0$. However, by convention, we assume $\\phi'(0)=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLu = ( lambda z : np.maximum(0,z),  # function\n",
    "         lambda z : z>0 )             # first derivative (assuming bool->int casting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5,5, 100)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5), facecolor='w', edgecolor='k')\n",
    "axs[0].plot(z, tanh[0](z),'blue', label=\"tanh\")\n",
    "axs[0].plot(z, tanh[1](z),'orange', label=\"tanh'\")\n",
    "axs[0].title.set_text('tanh')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(z, sigmoid[0](z),'blue', label=\"sigmoid\")\n",
    "axs[1].plot(z, sigmoid[1](z),'orange', label=\"sigmoid'\")\n",
    "axs[1].title.set_text('sigmoid')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(z, ReLu[0](z),'blue', label=\"ReLu\")\n",
    "axs[2].plot(z, ReLu[1](z),'orange', label=\"ReLu'\")\n",
    "axs[2].title.set_text('ReLu')\n",
    "axs[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcXQsA3EzEs9"
   },
   "source": [
    "Note that the range of output values for the derivative of the `sigmoid` function shrinks to a maximum of $0.25$, complicating the flow of gradients in backpropagation (#vanishing_gradients).\n",
    "\n",
    "Finally, for multi-class classification tasks, usually the activation function of the last layer is `softmax`:\n",
    "\n",
    "$$\\phi(z_i)= \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}}$$\n",
    "\n",
    "and its first derivative is: \n",
    "\n",
    "$$\\phi' (z)= \\text{softmax}'(z) =\\frac{\\partial \\phi}{\\partial z_j}(z_i)= \n",
    "\\begin{cases} \n",
    "\\phi(z_i)(1-\\phi(z_i)), & \\text{if } i=j \\\\ \n",
    "-\\phi(z_i)\\phi(z_j), & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = ( lambda z : (np.e**z)/np.sum((np.e**z)),                            # function\n",
    "            lambda z : np.diag(softmax[0](z))-softmax[0](z)*softmax[0](z).T )  # first derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcXQsA3EzEs9"
   },
   "source": [
    "## Building an ANN\n",
    "We now have all the components for building a neural network with two hidden layers with 3 neurons each and `tanh` activation, and a single-neuron output layer with `sigmoid` activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ann = ANN(X.shape[1], [3,3,1], [ReLu, ReLu, sigmoid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can even run it and get some predictions for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_ann.forward_propagation(X, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, **wait**! We are using random weights and bias terms in all the neurons. These predictions are just random guesses.\n",
    "\n",
    "We want to set these parameters. We call this the **learning** step, and for ANNs we use backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcXQsA3EzEs9"
   },
   "source": [
    "# Learning ANN's parameters\n",
    "\n",
    "We want the model's parameters that lead to the best possible solution. So, the first thing we need to set is how we decide what is the best solution. And we usually do that through a **loss function**: the best model is the one that minimizes the loss function. Note that this is also the objective function of an optimization problem.\n",
    "\n",
    "Once we have defined this, **backpropagation** is a method that allows us to update (improve) the model's parameters according to that objective (loss function). That is, it enables the application of a single *gradient descent* step.\n",
    "\n",
    "We start with randomly selected parameters. Then and until the model is good enough, backpropagation is repeatedly executed to calculate the gradients that allow us to compute the parameters' updates.\n",
    "\n",
    "## Loss function\n",
    "The loss function gives a score to each parametrized model. We can compare different models (different sets of parameters for our ANN) by comparing their losses.\n",
    "\n",
    "There are many loss functions and they are different depending on the task (regression or classification). Let's define the most typical ones. \n",
    "\n",
    "The **Mean Squared Error** (MSE) is a **regression** loss function defined as:\n",
    "\n",
    "$$\\mathcal{L}_{MSE}(\\hat{\\mathbf{y}},\\mathbf{y}) = \\frac{1}{n}\\sum_{i = 1}^{n}\\left ( \\hat{y}_{i} - y_{i} \\right )^{2}/2$$\n",
    "\n",
    "where we include a constant division by 2 (which does not modify the objective function) that simplifies the gradient. Its derivative with respect to $\\hat{\\mathbf{y}}$ is:\n",
    "\n",
    "$$\\mathcal{L}_{MSE}'(\\hat{\\mathbf{y}},\\mathbf{y}) = \\frac{\\partial \\mathcal{L}_{MSE}}{\\partial \\hat{y}_{i}}(\\hat{\\mathbf{y}},\\mathbf{y})=  \\hat{y}_{i} - y_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = ( lambda yr, yh: np.mean(((yh-yr)**2)/2.), \n",
    "             lambda yr, yh: (yh-yr) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcXQsA3EzEs9"
   },
   "source": [
    "The **cross-entropy** (CE) is a **classification** loss function defined as:\n",
    "\n",
    "$$\\mathcal{L}_{CE}(\\hat{\\mathbf{Y}},\\mathbf{Y}) = -\\frac{1}{n}\\sum_{i = 1}^{n}\\sum_{j = 1}^{m}Y_{i,j}\\log\\hat{Y}_{i,j}$$\n",
    "\n",
    "assuming that each $Y_{i,\\cdot}$ is a $k$-sized vector such that $\\sum_{j=1}^kY_{i,j}=1$ and $\\forall j, Y_{i,j}\\geq 0$. That is a standard output of a probabilistic classifier in multi-class classification. The derivative with respect to any component $\\hat{Y}_{i,j}$ is:\n",
    "\n",
    "$$\\mathcal{L}_{CE}'(\\hat{\\mathbf{Y}},\\mathbf{Y}) =\\frac{\\partial \\mathcal{L}_{CE}}{\\partial \\hat{Y}_{ij}}(\\hat{\\mathbf{Y}},\\mathbf{Y})= -\\frac{Y_{i,j}}{\\hat{Y}_{i,j}}$$\n",
    "\n",
    "In a binary classification task our output is usually a single probability value, $y_i$ for instance $i$. Specifically, $y_i$ is the probability of the positive class for instance $i$, whereas the probability of the negative class would be $1-y_i$. With this in mind, the **binary cross-entropy** (BCE) has a simpler definition:\n",
    "$$\\mathcal{L}_{BCE}(\\hat{\\mathbf{y}},\\mathbf{y}) = - \\frac{1}{n}\\sum_{i = 1}^{n}\\left ( y_{i}\\log\\hat{y}_{i}+(1-y_{i})\\log(1-\\hat{y}_{i}) \\right )$$\n",
    "and its derivative is:\n",
    "\n",
    "$$\\mathcal{L}_{BCE}'(\\hat{\\mathbf{y}},\\mathbf{y}) = \\frac{\\partial \\mathcal{L}_{CE}}{\\partial \\hat{y}_{i}}(\\hat{\\mathbf{y}},\\mathbf{y})= - \\frac{y_{i}}{\\hat{y}_{i}}+\\frac{1-y_{i}}{1-\\hat{y}_{i}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2170,
     "status": "ok",
     "timestamp": 1614301544367,
     "user": {
      "displayName": "Luis Cruz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAdeJfpdmWlDnaYzYy_nbUNs-8KmTX4RuBuPQbP_4=s64",
      "userId": "01516691432947049938"
     },
     "user_tz": 360
    },
    "id": "b1-0l2OrlQ2d"
   },
   "outputs": [],
   "source": [
    "ce_loss = ( lambda yr, yh: - np.mean(np.sum(yr*np.log(yh)),axis=-1), \n",
    "            lambda yr, yh: - np.sum(yr/yh,axis=-1) )\n",
    "\n",
    "bce_loss = ( lambda yr, yh: - np.mean(yr*np.log(yh)+(1-yr)*np.log(1-yh)), \n",
    "             lambda yr, yh: -yr/yh + (1-yr)/(1-yh))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Now that we have defined the concept of ANN and the loss function, we can define the algorithm of **backpropagation**. It allows us to perform a single update step for the ANN's parameters, as an iteration of GD does.\n",
    "The algorithm applies the *chain rule*: given a composition of functions $f(g(h(x)))$, the derivative $df/dx$ is given by $$\\frac{df}{dx}=\\frac{df}{dg}\\cdot \\frac{dg}{dh} \\cdot \\frac{dh}{dx}$$\n",
    "\n",
    "Thus, for an output's layer linear combination's parameter $w_{k1}$, the derivative of the objective function (loss) with respect to $w_{k1}$ is:\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial w_{k1}}=\\frac{\\partial\\mathcal{L}}{\\partial a^{(l)}_{1}} \\cdot \\frac{\\partial a^{(l)}_{1}}{\\partial z^{(l)}_{1}} \\cdot \\frac{\\partial z^{(l)}_{1}}{\\partial w^{(l)}_{k1}}$$\n",
    "\n",
    "where $z^{(l)}_1$ is the output of the linear combination performed in the output layer (assuming single-neuron layer) and $a^{(l)}_1$ is the output of the corresponding non-linear activation function (note that this will be our prediction, $a^{(l)}_1=\\hat{y}$). \n",
    "\n",
    "For parameters from other layers, we need to keep applying the chain rule backward (back-propagating the gradient). For these calculations, we need the partial derivatives of each function involved, as well as the partial results of the previous forward propagation step that allowed us to compute the loss. And, for sure, you need the parameters that you'd like to update.\n",
    "\n",
    "In **matrix form**, as this will be performed for multiple weights, neurons and, possibly, for multiple input samples too, it can be described as:\n",
    "$$\\nabla\\mathbf{W}^{(l)} =\\frac{\\partial\\mathcal{L}}{\\partial \\mathbf{A}^{(l)}} \\cdot \\frac{\\partial \\mathbf{A}^{(l)}}{\\partial \\mathbf{Z}^{(l)}} \\cdot \\frac{\\partial \\mathbf{Z}^{(l)}}{\\partial \\mathbf{W}^{(l)}}$$\n",
    "\n",
    "which can be shown to be:\n",
    "\n",
    "$$\\nabla\\mathbf{W}^{(l)}=\\frac{1}{N}(\\mathbf{y}-\\mathbf{A}^{(l)}) \\cdot \n",
    "\\left\\{\\!\\begin{aligned}\n",
    "&1 &\\text{ if } \\mathbf{Z}^{(l)}>0\\\\\n",
    "&0 &\\text{ if } \\mathbf{Z}^{(l)}\\leq 0\n",
    "\\end{aligned}\\right\\}\n",
    "\\cdot \\mathbf{A}^{(l-1)}$$\n",
    "\n",
    "if *mean squared error* is used as loss function and `ReLu` is used as activation function. And, for bias terms $\\mathbf{b}$ it is just:\n",
    "$$\\nabla\\mathbf{W}^{(l)}=\\frac{1}{N}(\\mathbf{y}-\\mathbf{A}^{(l)}) \\cdot \n",
    "\\left\\{\\!\\begin{aligned}\n",
    "&1 &\\text{ if } \\mathbf{Z}^{(l)}>0\\\\\n",
    "&0 &\\text{ if } \\mathbf{Z}^{(l)}\\leq 0\n",
    "\\end{aligned}\\right\\}\n",
    "\\cdot \\mathbf{1}$$\n",
    "\n",
    "\n",
    "For the previous layer, it can be described as:\n",
    "$$\\nabla\\mathbf{W}^{(l)} =\\frac{\\partial\\mathcal{L}}{\\partial \\mathbf{A}^{(l)}} \\cdot \n",
    "\\frac{\\partial \\mathbf{A}^{(l)}}{\\partial \\mathbf{Z}^{(l)}} \\cdot \n",
    "\\frac{\\partial \\mathbf{Z}^{(l)}}{\\partial \\mathbf{A}^{(l-1)}} \\cdot \n",
    "\\frac{\\partial \\mathbf{A}^{(l-1)}}{\\partial \\mathbf{Z}^{(l-1)}} \\cdot \n",
    "\\frac{\\partial \\mathbf{Z}^{(l-1)}}{\\partial \\mathbf{W}^{(l-1)}}$$\n",
    "\n",
    "\n",
    "which can be shown to be:\n",
    "\n",
    "$$\\nabla\\mathbf{W}^{(l)}=\\frac{1}{N}(\\mathbf{y}-\\mathbf{A}^{(l)}) \n",
    "\\cdot \\left\\{\\!\\begin{aligned}\n",
    "&1 &\\text{ if } \\mathbf{Z}^{(l)}>0\\\\\n",
    "&0 &\\text{ if } \\mathbf{Z}^{(l)}\\leq 0\n",
    "\\end{aligned}\\right\\}\n",
    "\\cdot \\mathbf{W}^{(l)} \n",
    "\\cdot \\left\\{\\!\\begin{aligned}\n",
    "&1 &\\text{ if } \\mathbf{Z}^{(l-1)}>0\\\\\n",
    "&0 &\\text{ if } \\mathbf{Z}^{(l-1)}\\leq 0\n",
    "\\end{aligned}\\right\\}\n",
    "\\cdot \\mathbf{A}^{(l-2)}$$\n",
    "\n",
    "**Note that** most of the computation is repeated for updating the different parameters. Thus, organizing wisely the calculations to avoid repetition can save you a lot of computation.\n",
    "\n",
    "In the following implementation, in each iteration we keep the gradient updated backward by the chain rule up to a specific layer. We compute the update of the corresponding layer's parameters and keep computing the gradient backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Performs backpropagation\n",
    "y: real output with dim=(n,1): where n is the number of cases\n",
    "loss_func: loss function and its derivative to be used\n",
    "lr: learning rate for gradient descent steps\n",
    "'''\n",
    "def back_propagation(self, y, loss_func, lr=0.01):\n",
    "    rl = len(self.intermediate_results) - 1\n",
    "    if rl < 0: \n",
    "        return None;\n",
    "    # we use two indices to track the layer because we store self.layers in 0..L-1 (L:num.layers), while we store \n",
    "    # their outputs self.intermediate_results in 1..L, with self.intermediate_results[0][1]=X (input data)\n",
    "    l = rl-1\n",
    "    # chain rule's first step: [d Loss / d A^l]\n",
    "    grad = loss_func[1](y,self.intermediate_results[rl][1]) \n",
    "    while (l >= 0):\n",
    "        # chain rule: product by [d A^l / d Z^l]\n",
    "        grad = grad * self.layers[l].act_f[1](self.intermediate_results[rl][0])\n",
    "\n",
    "        # To update weights W: product by [d Z^l / d W^l]\n",
    "        upd_W = (self.intermediate_results[rl-1][1].T @ grad)/grad.shape[0]\n",
    "        # To update bias terms b: product by [d Z^l / d b^l]\n",
    "        upd_b = np.mean(grad,axis=0,keepdims=True) # equivalent to np.ones((k,N))@grad/N\n",
    "        \n",
    "        # chain rule: product by [d Z^l / d A^(l-1)]\n",
    "        grad = grad @ self.layers[l].W.T\n",
    "\n",
    "        # gradient descent step\n",
    "        self.layers[l].W = #### YOUR CODE HERE ####\n",
    "        self.layers[l].b = #### YOUR CODE HERE ####\n",
    "\n",
    "        l-=1\n",
    "        rl-=1\n",
    "\n",
    "# add method to the ANN class\n",
    "ANN.back_propagation = back_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of vectors and matrices required in the **first iteration** can be graphically represented as:\n",
    "<img src=\"images/backpropagation_it1.png\" style=\"height:400px\"/>\n",
    "\n",
    "where the first step is to compute the gradient of the loss. The column-vector $\\mathbf{1}$ represents a size-$n$ vector of $1$'s. The **next iteration** is a fully hidden layer and the computation can be represented as:\n",
    "\n",
    "<img src=\"images/backpropagation_it2.png\" style=\"height:400px\"/>\n",
    "\n",
    "The **final iteration** of this 3-layer ANNs helps update the parameters of the first layer, the ones applied to the input data:\n",
    "\n",
    "<img src=\"images/backpropagation_it3.png\" style=\"height:400px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ann.back_propagation(y,bce_loss,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLPfuDcIp5-l"
   },
   "source": [
    "# Machine learning: training our artificial neural network\n",
    "\n",
    "We can build a method that groups all the steps required to perform the training:\n",
    "\n",
    "- Run **forward propagation** to get the intermediate results and the model's prediction for the training data.\n",
    "- Run **backpropagation** to update the model's parameters.\n",
    "- Inspect the performance of the learning process by:\n",
    "  - measuring the loss and accuracy in training and validation data\n",
    "  - observing the problem's feature space to visualize decision thresholds\n",
    "\n",
    "All this is repeated multiple times. Each iteration is called an **epoch** in deep learning terminology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def train(nn, X_train, y_train, X_test, y_test, loss_func, lr=0.01, epochs=1): \n",
    "    \n",
    "    # Data (50x50 grid) for visualizing currently learned concept\n",
    "    resolution=50\n",
    "    X0, X1 = np.meshgrid(np.linspace(-1.5,1.5,resolution), \n",
    "                         np.linspace(-1.5,1.5,resolution))\n",
    "    X_aux = np.vstack((X0.ravel(),X1.ravel())).T\n",
    "\n",
    "    loss= []\n",
    "    acc=[]\n",
    "    loss_test=[]\n",
    "    acc_test=[]\n",
    "    # Training loops (run through epochs)\n",
    "    for i in tqdm(range(epochs)):\n",
    "        # Run forward to calculate loss and intermediate results\n",
    "        res = nn.forward_propagation(X_train, True)\n",
    "        loss.append(loss_func[0](y_train, res))\n",
    "        y_pred=np.array([1 if x>0.5 else 0 for x in res]) \n",
    "        acc.append(np.sum(y_pred==y_train[:,0])/y_pred.shape[0])\n",
    "\n",
    "        # Run backpropagation to update model's parameters\n",
    "        nn.back_propagation(y_train, loss_func, lr)\n",
    "    \n",
    "        # We calculate the loss in validation data too for later inspection\n",
    "        res = nn.forward_propagation(X_test, False)\n",
    "        loss_test.append(loss_func[0](y_test,res))\n",
    "        y_pred=np.array([1 if x>0.5 else 0  for x in res]) \n",
    "        acc_test.append(np.sum(y_pred==y_test[:,0])/y_pred.shape[0])\n",
    "\n",
    "        if i % 50==0: # from time to time\n",
    "            # Visualize feature's space to display the currently learned concept\n",
    "            y_aux = my_ann.forward_propagation(X_aux, False)\n",
    "            y_aux = y_aux.reshape((resolution,resolution))\n",
    "            plt.pcolormesh(X0,X1,y_aux,cmap=\"coolwarm\")\n",
    "            plt.axis(\"equal\")\n",
    "            clear_output(wait=True)\n",
    "            plt.show()\n",
    "\n",
    "    return loss,loss_test,acc,acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "Now we just run the algorithm and visualize the results.\n",
    "\n",
    "We need to create the train/test split of our data, decide the topology of the ANN and run the `train` method.\n",
    "\n",
    "Then, we can show the results with different plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "executionInfo": {
     "elapsed": 120440,
     "status": "ok",
     "timestamp": 1614301662664,
     "user": {
      "displayName": "Luis Cruz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAdeJfpdmWlDnaYzYy_nbUNs-8KmTX4RuBuPQbP_4=s64",
      "userId": "01516691432947049938"
     },
     "user_tz": 360
    },
    "id": "tMvG-L7OuasU",
    "outputId": "6edf183a-122d-4c71-ce7a-f9bc88700ca0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rnd_generator = np.random.default_rng(17)\n",
    "\n",
    "# set up our ANN architecture\n",
    "my_ann = ANN(X.shape[1], [5,1], [tanh, sigmoid], rnd_generator)\n",
    "\n",
    "# split train/validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 13, shuffle=True, stratify=y)\n",
    "\n",
    "# let's train our ANN\n",
    "loss,loss_test,acc,acc_test= train(my_ann,X_train,y_train,X_test,y_test,bce_loss,lr=0.02,epochs=10000) #Entrenament\n",
    "\n",
    "# Visualize learning curves (train and validation data) for loss and accuracy\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5), facecolor='w', edgecolor='k')\n",
    "axs[0].plot(range(len(loss)), loss, 'tab:cyan', label=\"Train loss\")\n",
    "axs[0].plot(range(len(loss_test)), loss_test, 'tab:brown', label=\"Validation loss\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Binary Cross Entropy\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(range(len(acc)), acc, 'tab:cyan', label=\"Train accuracy\")\n",
    "axs[1].plot(range(len(acc_test)), acc_test, 'tab:brown', label=\"Validation accuracy\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].legend()\n",
    "plt.show()\n",
    "\n",
    "# Compare training data's predicted and real outputs\n",
    "y_pred = my_ann.forward_propagation(X,False)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5), facecolor='w', edgecolor='k')\n",
    "\n",
    "axs[0].scatter(X[:,0],X[:,1],c=y_pred>0.5)\n",
    "axs[0].set_title(\"Predicted outputs\")\n",
    "axs[1].scatter(X[:,0],X[:,1],c=y)\n",
    "axs[1].set_title(\"Real outputs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- In the results, in the learning curve plots, we observe a constant reduction of the loss function, but accuracy does not change as smoothly. In fact, accuracy in validation finally degrades again. Why? Can you prevent this from happening?\n",
    "- Can you build a stochastic gradient descent method?\n",
    "- We can use different `sklearn` methods to create synthetic data (moons o blobbs) with more than 2 possible clusters (classes). Modify what is required to make multiclass classification (`softmax` as activation in the last layer and a loss function for multi-class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IA_Notebook _4_Programando_Red_Neuronal_desde_Cero.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
